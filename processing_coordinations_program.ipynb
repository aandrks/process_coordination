{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aandrks/process_coordination/blob/main/processing_coordinations_program.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Swu9KQiCrsjM",
        "outputId": "7ff69285-7db6-4a3a-ab7b-9911574a6340"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fatal Python error: init_import_site: Failed to import the site module\n",
            "Python runtime state: initialized\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in exec_module\n",
            "  File \"<frozen site>\", line 652, in <module>\n",
            "  File \"<frozen site>\", line 639, in main\n",
            "  File \"<frozen site>\", line 421, in addsitepackages\n",
            "  File \"<frozen site>\", line 253, in addsitedir\n",
            "  File \"<frozen site>\", line 212, in addpackage\n",
            "object address  : 0x7fc0f9802260\n",
            "object refcount : 1\n",
            "object type     : 0xa2a4e0\n",
            "object type name: KeyboardInterrupt\n",
            "object repr     : KeyboardInterrupt()\n",
            "lost sys.stderr\n",
            "  File \"<string>\", line 1, in <module>\n",
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.12/dist-packages (3.2.9)\n",
            "=== Coordination Processing System ===\n",
            "\n",
            "Select mode:\n",
            "1. Data Loading - Process employee data and update database\n",
            "2. Data Matching - Process coordination data using existing database\n",
            "3. Upload employee database JSON file\n",
            "4. Exit\n",
            "Enter your choice (1-4): 1\n",
            "=== Data Loading Mode ===\n",
            "This mode processes employee data and updates the database.\n",
            "✓ Loaded employee database with 21 employees and 8 companies\n",
            "\n",
            "Upload company and employee data file (CSV or Excel):\n",
            "Please upload company and employee data file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3803941a-d589-4a5d-b38d-161bab676f95\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3803941a-d589-4a5d-b38d-161bab676f95\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving reviews_export-31-10-2025.xlsx to reviews_export-31-10-2025 (1).xlsx\n",
            "✓ Uploaded: reviews_export-31-10-2025 (1).xlsx\n",
            "\n",
            "Processing company and employee data...\n",
            "==================================================\n",
            "✓ Saved employee database with 21 employees and 8 companies\n",
            "Saved data_loading_log.txt\n",
            "\n",
            "Data loading completed successfully!\n",
            "Database now contains 21 employees and 8 companies\n",
            "\n",
            "Select mode:\n",
            "1. Data Loading - Process employee data and update database\n",
            "2. Data Matching - Process coordination data using existing database\n",
            "3. Upload employee database JSON file\n",
            "4. Exit\n",
            "Enter your choice (1-4): 2\n",
            "=== Data Matching Mode ===\n",
            "This mode processes coordination data using the existing database.\n",
            "✓ Loaded employee database with 21 employees and 8 companies\n",
            "\n",
            "Upload coordination data file (CSV or Excel):\n",
            "Please upload coordination data file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5a88df85-6d8a-4dae-aac2-5cb4de3f748d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5a88df85-6d8a-4dae-aac2-5cb4de3f748d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving reviews_export-31-10-2025.xlsx to reviews_export-31-10-2025 (2).xlsx\n",
            "✓ Uploaded: reviews_export-31-10-2025 (2).xlsx\n",
            "\n",
            "\n",
            "\n",
            "Type date in format dd.mm.YYYY \n",
            "or pull Enter if today is ok: \n",
            "\n",
            "Processing coordinations (as of 2025-11-07)... 2025-11-07 17:54:54.108206\n",
            "Using 'id' as coordination ID\n",
            "\n",
            "==================================================\n",
            "Coordination Processing Details (Enhanced Matching):\n",
            "==================================================\n",
            "ID: 282622\n",
            "  Company: bim-info\n",
            "  Start Date: 2025-10-30\n",
            "  Deadline: 2025-11-04\n",
            "  Working Days: 3\n",
            "  Explanation: Stage 2: not configured, using default 3 days\n",
            "  Not Checked Count: 1\n",
            "  Emails: akolosova@bim-info.ru\n",
            "--------------------------------------------------\n",
            "ID: 280308\n",
            "  Company: su-19, fodd, c-and-u\n",
            "  Start Date: 2025-10-29\n",
            "  Deadline: 2025-11-05\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 3\n",
            "  Emails: dkornakov@fodd.ru, mzemtsov@c-and-u.co, kiseleva@su-19.com\n",
            "--------------------------------------------------\n",
            "ID: 280301\n",
            "  Company: su-19, fodd, c-and-u\n",
            "  Start Date: 2025-10-29\n",
            "  Deadline: 2025-11-05\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 3\n",
            "  Emails: dkornakov@fodd.ru, mzemtsov@c-and-u.co, cvetkov@su-19.com\n",
            "--------------------------------------------------\n",
            "ID: 278635\n",
            "  Company: su-19, fodd\n",
            "  Start Date: 2025-10-26\n",
            "  Deadline: 2025-10-31\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 3\n",
            "  Emails: petrov@su-19.com, ikirin@fodd.ru\n",
            "--------------------------------------------------\n",
            "ID: 277308\n",
            "  Company: su-19, fodd, c-and-u\n",
            "  Start Date: 2025-10-24\n",
            "  Deadline: 2025-10-31\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 3\n",
            "  Emails: dkornakov@fodd.ru, mzemtsov@c-and-u.co, kiseleva@su-19.com\n",
            "--------------------------------------------------\n",
            "ID: 277282\n",
            "  Company: su-19, fodd, c-and-u\n",
            "  Start Date: 2025-10-24\n",
            "  Deadline: 2025-10-31\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 3\n",
            "  Emails: dkornakov@fodd.ru, mzemtsov@c-and-u.co, kiseleva@su-19.com\n",
            "--------------------------------------------------\n",
            "ID: 277280\n",
            "  Company: su-19, fodd, c-and-u\n",
            "  Start Date: 2025-10-24\n",
            "  Deadline: 2025-10-31\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 3\n",
            "  Emails: dkornakov@fodd.ru, mzemtsov@c-and-u.co, kiseleva@su-19.com\n",
            "--------------------------------------------------\n",
            "ID: 277202\n",
            "  Company: su-19, c-and-u\n",
            "  Start Date: 2025-10-24\n",
            "  Deadline: 2025-10-31\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 2\n",
            "  Emails: mzemtsov@c-and-u.co, cvetkov@su-19.com\n",
            "--------------------------------------------------\n",
            "ID: 276820\n",
            "  Company: su-19, fodd, mr-group, c-and-u\n",
            "  Start Date: 2025-10-23\n",
            "  Deadline: 2025-10-30\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 4\n",
            "  Emails: petrov@su-19.com, mzemtsov@c-and-u.co, ikirin@fodd.ru, lapteva_y@mr-group.ru\n",
            "--------------------------------------------------\n",
            "ID: 276629\n",
            "  Company: su-19, fodd\n",
            "  Start Date: 2025-10-22\n",
            "  Deadline: 2025-10-29\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 3\n",
            "  Emails: petrov@su-19.com, ikirin@fodd.ru\n",
            "--------------------------------------------------\n",
            "ID: 275810\n",
            "  Company: su-19, fodd\n",
            "  Start Date: 2025-10-21\n",
            "  Deadline: 2025-10-28\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 3\n",
            "  Emails: petrov@su-19.com, ikirin@fodd.ru\n",
            "--------------------------------------------------\n",
            "ID: 275651\n",
            "  Company: su-19, fodd, c-and-u\n",
            "  Start Date: 2025-10-21\n",
            "  Deadline: 2025-10-28\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 3\n",
            "  Emails: dkornakov@fodd.ru, mzemtsov@c-and-u.co, kiseleva@su-19.com\n",
            "--------------------------------------------------\n",
            "ID: 275378\n",
            "  Company: su-19, fodd, c-and-u\n",
            "  Start Date: 2025-10-21\n",
            "  Deadline: 2025-10-28\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 3\n",
            "  Emails: dkornakov@fodd.ru, mzemtsov@c-and-u.co, cvetkov@su-19.com\n",
            "--------------------------------------------------\n",
            "ID: 275361\n",
            "  Company: fodd\n",
            "  Start Date: 2025-10-19\n",
            "  Deadline: 2025-10-24\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 2\n",
            "  Emails: ikirin@fodd.ru\n",
            "--------------------------------------------------\n",
            "ID: 274324\n",
            "  Company: su-19, fodd, mr-group, c-and-u\n",
            "  Start Date: 2025-10-19\n",
            "  Deadline: 2025-10-24\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 4\n",
            "  Emails: petrov@su-19.com, mzemtsov@c-and-u.co, ikirin@fodd.ru, lapteva_y@mr-group.ru\n",
            "--------------------------------------------------\n",
            "ID: 272368\n",
            "  Company: fodd\n",
            "  Start Date: 2025-10-13\n",
            "  Deadline: 2025-10-20\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 2\n",
            "  Emails: ikirin@fodd.ru\n",
            "--------------------------------------------------\n",
            "ID: 271653\n",
            "  Company: fodd\n",
            "  Start Date: 2025-10-12\n",
            "  Deadline: 2025-10-17\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 2\n",
            "  Emails: ikirin@fodd.ru\n",
            "--------------------------------------------------\n",
            "ID: 271425\n",
            "  Company: su-19, fodd, mr-group\n",
            "  Start Date: 2025-10-13\n",
            "  Deadline: 2025-10-20\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 3\n",
            "  Emails: petrov@su-19.com, ikirin@fodd.ru, lapteva_y@mr-group.ru\n",
            "--------------------------------------------------\n",
            "ID: 270341\n",
            "  Company: su-19, fodd, c-and-u\n",
            "  Start Date: 2025-10-12\n",
            "  Deadline: 2025-10-17\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 3\n",
            "  Emails: dkornakov@fodd.ru, mzemtsov@c-and-u.co, cvetkov@su-19.com\n",
            "--------------------------------------------------\n",
            "ID: 270257\n",
            "  Company: su-19\n",
            "  Start Date: 2025-10-09\n",
            "  Deadline: 2025-10-16\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 1\n",
            "  Emails: cvetkov@su-19.com\n",
            "--------------------------------------------------\n",
            "ID: 270256\n",
            "  Company: su-19\n",
            "  Start Date: 2025-10-12\n",
            "  Deadline: 2025-10-17\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 1\n",
            "  Emails: cvetkov@su-19.com\n",
            "--------------------------------------------------\n",
            "ID: 269520\n",
            "  Company: su-19\n",
            "  Start Date: 2025-10-09\n",
            "  Deadline: 2025-10-16\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 1\n",
            "  Emails: cvetkov@su-19.com\n",
            "--------------------------------------------------\n",
            "ID: 264920\n",
            "  Company: su-19, fodd\n",
            "  Start Date: 2025-09-29\n",
            "  Deadline: 2025-10-06\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 2\n",
            "  Emails: dkornakov@fodd.ru, kiseleva@su-19.com\n",
            "--------------------------------------------------\n",
            "ID: 262908\n",
            "  Company: su-19\n",
            "  Start Date: 2025-09-24\n",
            "  Deadline: 2025-10-01\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 1\n",
            "  Emails: cvetkov@su-19.com\n",
            "--------------------------------------------------\n",
            "ID: 261803\n",
            "  Company: su-19\n",
            "  Start Date: 2025-09-23\n",
            "  Deadline: 2025-09-30\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 1\n",
            "  Emails: cvetkov@su-19.com\n",
            "--------------------------------------------------\n",
            "ID: 261787\n",
            "  Company: su-19\n",
            "  Start Date: 2025-09-22\n",
            "  Deadline: 2025-09-29\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 1\n",
            "  Emails: kiseleva@su-19.com\n",
            "--------------------------------------------------\n",
            "ID: 261778\n",
            "  Company: su-19\n",
            "  Start Date: 2025-09-22\n",
            "  Deadline: 2025-09-29\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 1\n",
            "  Emails: cvetkov@su-19.com\n",
            "--------------------------------------------------\n",
            "ID: 259218\n",
            "  Company: su-19\n",
            "  Start Date: 2025-09-16\n",
            "  Deadline: 2025-09-23\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 1\n",
            "  Emails: kiseleva@su-19.com\n",
            "--------------------------------------------------\n",
            "ID: 257898\n",
            "  Company: su-19\n",
            "  Start Date: 2025-09-14\n",
            "  Deadline: 2025-09-19\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 1\n",
            "  Emails: kiseleva@su-19.com\n",
            "--------------------------------------------------\n",
            "ID: 257185\n",
            "  Company: c-and-u\n",
            "  Start Date: 2025-09-11\n",
            "  Deadline: 2025-09-18\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 1\n",
            "  Emails: mzemtsov@c-and-u.co\n",
            "--------------------------------------------------\n",
            "ID: 238734\n",
            "  Company: su-19\n",
            "  Start Date: 2025-07-27\n",
            "  Deadline: 2025-08-01\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 1\n",
            "  Emails: cvetkov@su-19.com\n",
            "--------------------------------------------------\n",
            "ID: 227844\n",
            "  Company: su-19\n",
            "  Start Date: 2025-07-01\n",
            "  Deadline: 2025-07-08\n",
            "  Working Days: 5\n",
            "  Explanation: Stage 3: not configured, using default 5 days\n",
            "  Not Checked Count: 1\n",
            "  Emails: cvetkov@su-19.com\n",
            "--------------------------------------------------\n",
            "Saved results.txt\n",
            "Saved overdue_emails.txt\n",
            "Saved overdue_coordination_ids.txt\n",
            "Saved coordination_details.txt\n",
            "Saved coordination_details.xlsx\n",
            "Saved debug_info.txt\n",
            "Saved matching_log.txt\n",
            "\n",
            "✓ Results saved:\n",
            "- results.txt (company statistics)\n",
            "- overdue_emails.txt (reminder emails)\n",
            "- overdue_coordination_ids.txt (overdue coordination IDs)\n",
            "- coordination_details.txt (detailed coordination info)\n",
            "- coordination_details.xlsx (Excel file with coordination details)\n",
            "- debug_info.txt (processing debug information)\n",
            "- matching_log.txt (detailed matching process)\n",
            "\n",
            "Final results:\n",
            "==================================================\n",
            "Overdue coordination count by company:\n",
            "- su-19: 27\n",
            "- fodd: 18\n",
            "- c-and-u: 12\n",
            "- mr-group: 3\n",
            "- bim-info: 1\n",
            "\n",
            "Overdue coordination IDs: 282622, 280308, 280301, 278635, 277308, 277282, 277280, 277202, 276820, 276629, 275810, 275651, 275378, 275361, 274324, 272368, 271653, 271425, 270341, 270257, 270256, 269520, 264920, 262908, 261803, 261787, 261778, 259218, 257898, 257185, 238734, 227844\n",
            "\n",
            "Data matching completed successfully!\n",
            "\n",
            "Select mode:\n",
            "1. Data Loading - Process employee data and update database\n",
            "2. Data Matching - Process coordination data using existing database\n",
            "3. Upload employee database JSON file\n",
            "4. Exit\n"
          ]
        }
      ],
      "source": [
        "# @title Coordinations Processing\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import os\n",
        "import traceback\n",
        "import unicodedata\n",
        "!pip install rapidfuzz\n",
        "from rapidfuzz import fuzz, process\n",
        "from google.colab import files\n",
        "!pip install xlsxwriter\n",
        "\n",
        "# Configuration - use relative paths for Colab\n",
        "EMPLOYEE_DB_FILE = 'employee_database.json'\n",
        "public_domains = {'mail', 'yandex', 'gmail', 'yahoo', 'hotmail', 'outlook'}\n",
        "no_match_array = []\n",
        "holidays = ['01-01', '02-01', '03-01', '04-01', '05-01', '06-01', '07-01', '23-02', '08-03', '01-05', '09-05', '12-06', '03-11', '04-11'] # ADD HOLIDAYS\n",
        "working_holidays = ['01-11'] # ADD EXTRA WORKING DAYS\n",
        "\n",
        "\n",
        "def load_employee_db():\n",
        "    \"\"\"Load employee database from file\"\"\"\n",
        "    try:\n",
        "        if Path(EMPLOYEE_DB_FILE).exists():\n",
        "            with open(EMPLOYEE_DB_FILE, 'r', encoding='utf-8') as f:\n",
        "                db = json.load(f)\n",
        "                # Convert companies list back to set\n",
        "                db['companies'] = set(db['companies'])\n",
        "                print(\n",
        "                    f\"✓ Loaded employee database with {len(db['employees'])} employees and {len(db['companies'])} companies\")\n",
        "                return db\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error loading employee database: {e}\")\n",
        "    return {'employees': [], 'companies': set()}\n",
        "\n",
        "\n",
        "def save_employee_db(db):\n",
        "    \"\"\"Save employee database to file\"\"\"\n",
        "    try:\n",
        "        # Convert sets to lists for JSON serialization\n",
        "        db_to_save = {\n",
        "            'employees': db['employees'],\n",
        "            'companies': list(db['companies'])\n",
        "        }\n",
        "        with open(EMPLOYEE_DB_FILE, 'w', encoding='utf-8') as f:\n",
        "            json.dump(db_to_save, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"✓ Saved employee database with {len(db['employees'])} employees and {len(db['companies'])} companies\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error saving employee database: {e}\")\n",
        "\n",
        "\n",
        "def upload_file(file_type=\"any\"):\n",
        "    \"\"\"Upload a file in Google Colab\"\"\"\n",
        "    print(f\"Please upload {file_type} file:\")\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        filename = list(uploaded.keys())[0]\n",
        "        print(f\"✓ Uploaded: {filename}\")\n",
        "        return filename\n",
        "    else:\n",
        "        print(\"No file uploaded\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"Normalize text by removing accents, special characters, and converting to lowercase\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove accents and diacritics\n",
        "    text = unicodedata.normalize('NFKD', text)\n",
        "    text = ''.join([c for c in text if not unicodedata.combining(c)])\n",
        "\n",
        "    # Remove all non-alphanumeric characters except spaces and dots (for initials)\n",
        "    text = re.sub(r'[^\\w\\s.]', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    return text.lower().strip()\n",
        "\n",
        "\n",
        "def is_initial(part):\n",
        "    \"\"\"Check if a name part is an initial\"\"\"\n",
        "    return len(part) <= 2 or (len(part) == 2 and part.endswith('.'))\n",
        "\n",
        "\n",
        "def extract_name_components(name):\n",
        "    \"\"\"Extract surname and given names from a full name with proper handling of initials\"\"\"\n",
        "    if not isinstance(name, str):\n",
        "        return \"\", \"\"\n",
        "\n",
        "    # Remove any non-alphabetic characters except spaces and dots\n",
        "    clean_name = re.sub(r'[^а-яА-ЯёЁa-zA-Z\\s.]', '', name).strip()\n",
        "\n",
        "    # Handle comma-separated names (Last, First format)\n",
        "    if ',' in clean_name:\n",
        "        parts = [p.strip() for p in clean_name.split(',')]\n",
        "        if len(parts) >= 2:\n",
        "            return parts[0], parts[1]  # surname, given names\n",
        "\n",
        "    # Split into words and remove empty parts\n",
        "    parts = [p for p in re.split(r'\\s+', clean_name) if p]\n",
        "\n",
        "    if not parts:\n",
        "        return \"\", \"\"\n",
        "    if len(parts) == 1:\n",
        "        return parts[0], \"\"\n",
        "\n",
        "    # Check if the last part is an initial\n",
        "    if is_initial(parts[-1]):\n",
        "        # Format: \"Surname Initial\" or \"Surname I.\"\n",
        "        return parts[0], parts[-1]\n",
        "\n",
        "    # Check if the first part is an initial\n",
        "    if is_initial(parts[0]):\n",
        "        # Format: \"Initial Surname\" or \"I. Surname\"\n",
        "        return parts[-1], parts[0]\n",
        "\n",
        "    # If no initials, assume last word is surname\n",
        "    return parts[-1], \" \".join(parts[:-1])\n",
        "\n",
        "\n",
        "def parse_company_person_data(file_content, db):\n",
        "    \"\"\"Process company and employee data with enhanced name handling and team support\"\"\"\n",
        "    company_person_map = defaultdict(list)\n",
        "    new_employees = []\n",
        "    seen_emails = {e['email'] for e in db['employees']}\n",
        "    processing_log = []\n",
        "    team_id_counter = 1  # Counter for generating unique team IDs\n",
        "\n",
        "    print(\"\\nProcessing company and employee data...\")\n",
        "    print(\"=\" * 50)\n",
        "    processing_log.append(\"Processing company and employee data...\")\n",
        "    processing_log.append(\"=\" * 50)\n",
        "\n",
        "    for line_num, line in enumerate(file_content.split('\\n'), 1):\n",
        "        if not line.strip():\n",
        "            continue\n",
        "\n",
        "        original_line = line.strip()\n",
        "        lines = file_content.split('\\n')\n",
        "\n",
        "    for line_num in range(len(lines)):\n",
        "        line = lines[line_num].strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        # NEW: Check if this line ends with / and combine with next line\n",
        "        if line.endswith('/') and line_num + 1 < len(lines):\n",
        "            next_line = lines[line_num + 1].strip()\n",
        "            if next_line:\n",
        "                line = line.rstrip('/') + ' ' + next_line\n",
        "                # Skip the next line since we've combined it\n",
        "                lines[line_num + 1] = \"\"\n",
        "        original_line = line\n",
        "        processing_log.append(f\"\\nLine {line_num + 1}: {original_line}\")\n",
        "\n",
        "        # Find all email blocks in parentheses\n",
        "        for block in re.findall(r'(?:\\(| - )([^()]+?\\s+[^\\s@]+@[^\\s/@]+(?:\\s*/\\s*[^()]+?\\s+[^\\s@]+@[^\\s/@]+)*)', line):\n",
        "            if '@' not in block:\n",
        "                continue\n",
        "\n",
        "            processing_log.append(f\"  Found email block: {block}\")\n",
        "\n",
        "            # Extract team members (people separated by '/')\n",
        "            team_members = [p.strip() for p in block.split('/')]\n",
        "            team_id = f\"team_{team_id_counter}\"\n",
        "            team_id_counter += 1\n",
        "\n",
        "            # Process each person in the team\n",
        "            team_company = None\n",
        "            team_emails = []\n",
        "\n",
        "            for person in team_members:\n",
        "                match = re.search(r'([^@]+)\\s+([^\\s@]+@[^\\s@]+)', person)\n",
        "                if not match:\n",
        "                    processing_log.append(f\"    Skipping malformed person entry: {person}\")\n",
        "                    continue\n",
        "\n",
        "                name, email = match.group(1).strip(), match.group(2).strip()\n",
        "                email = re.sub(r'[),.;]+$', '', email)\n",
        "                email = email.strip()\n",
        "                processing_log.append(f\"    Processing: {name} <{email}>\")\n",
        "\n",
        "                if email in seen_emails:\n",
        "                    processing_log.append(f\"    Skipping duplicate email: {email}\")\n",
        "                    continue\n",
        "                seen_emails.add(email)\n",
        "                team_emails.append(email)\n",
        "\n",
        "                domain = email.split('@')[-1].split('.')[0]\n",
        "                surname, given_names = extract_name_components(name)\n",
        "                normalized_name = normalize_text(name)\n",
        "\n",
        "                processing_log.append(f\"    Extracted: surname='{surname}', given_names='{given_names}'\")\n",
        "\n",
        "                if domain in public_domains:\n",
        "                    processing_log.append(f\"    Public domain detected: {domain}\")\n",
        "                    print(f\"\\nFound public domain email:\")\n",
        "                    print(f\"Original line: {original_line}\")\n",
        "                    print(f\"Employee: {name} <{email}>\")\n",
        "\n",
        "                    while True:\n",
        "                        company = input(\"Assign company (leave blank to skip, 'q' to quit): \").strip()\n",
        "\n",
        "                        if company.lower() == 'q':\n",
        "                            # Save before quitting\n",
        "                            db['employees'].extend(new_employees)\n",
        "                            db['companies'].update(e['company'] for e in new_employees)\n",
        "                            save_employee_db(db)\n",
        "                            processing_log.append(\"User quit during manual assignment\")\n",
        "                            return db, company_person_map, processing_log\n",
        "\n",
        "                        if not company:\n",
        "                            processing_log.append(f\"    Skipped {email}\")\n",
        "                            print(f\"Skipped {email}\")\n",
        "                            break\n",
        "\n",
        "                        if len(company) < 2:\n",
        "                            print(\"Company name too short, try again\")\n",
        "                            continue\n",
        "\n",
        "                        # If this is the first team member with a manually assigned company,\n",
        "                        # use this company for the whole team\n",
        "                        if team_company is None:\n",
        "                            team_company = company\n",
        "                            db['companies'].add(company)\n",
        "\n",
        "                        new_employees.append({\n",
        "                            'name': name,\n",
        "                            'email': email,\n",
        "                            'normalized_name': normalized_name,\n",
        "                            'surname': surname,\n",
        "                            'given_names': given_names,\n",
        "                            'company': company,\n",
        "                            'source': 'manual',\n",
        "                            'team_id': team_id,\n",
        "                            'team_emails': team_emails\n",
        "                        })\n",
        "                        company_person_map[company].append({\n",
        "                            'name': name,\n",
        "                            'email': email,\n",
        "                            'normalized_name': normalized_name,\n",
        "                            'surname': surname,\n",
        "                            'given_names': given_names,\n",
        "                            'team_id': team_id,\n",
        "                            'team_emails': team_emails\n",
        "                        })\n",
        "                        processing_log.append(f\"    Assigned to company: {company}\")\n",
        "                        print(f\"✓ Assigned: {company}\")\n",
        "                        break\n",
        "                else:\n",
        "                    # If this is the first team member with a company domain,\n",
        "                    # use this company for the whole team\n",
        "                    if team_company is None:\n",
        "                        team_company = domain\n",
        "                        db['companies'].add(domain)\n",
        "\n",
        "                    new_employees.append({\n",
        "                        'name': name,\n",
        "                        'email': email,\n",
        "                        'normalized_name': normalized_name,\n",
        "                        'surname': surname,\n",
        "                        'given_names': given_names,\n",
        "                        'company': domain,\n",
        "                        'source': 'auto',\n",
        "                        'team_id': team_id,\n",
        "                        'team_emails': team_emails\n",
        "                    })\n",
        "                    company_person_map[domain].append({\n",
        "                        'name': name,\n",
        "                        'email': email,\n",
        "                        'normalized_name': normalized_name,\n",
        "                        'surname': surname,\n",
        "                        'given_names': given_names,\n",
        "                        'team_id': team_id,\n",
        "                        'team_emails': team_emails\n",
        "                    })\n",
        "                    processing_log.append(f\"    Auto-assigned to domain: {domain}\")\n",
        "                    print(f\"✓ Auto-assigned: {name} <{email}> → {domain}\")\n",
        "\n",
        "    # Update database with new employees\n",
        "    db['employees'].extend(new_employees) # extending\n",
        "    # db['employees'] = new_employees # exchanging\n",
        "    # change type of loading data update\n",
        "    save_employee_db(db)\n",
        "    processing_log.append(f\"\\nAdded {len(new_employees)} new employees to database\")\n",
        "\n",
        "    return db, company_person_map, processing_log\n",
        "\n",
        "\n",
        "def find_best_match(target_name, candidates, debug_info=None):\n",
        "    \"\"\"Find the best match using fuzzy matching on full name\"\"\"\n",
        "    if debug_info is None:\n",
        "        debug_info = []\n",
        "\n",
        "    debug_info.append(f\"    Finding best match for: {target_name}\")\n",
        "    debug_info.append(f\"    Candidates: {[c['name'] for c in candidates]}\")\n",
        "\n",
        "    # Extract components from target name for potential exact surname matching\n",
        "    target_surname, target_given = extract_name_components(target_name)\n",
        "    debug_info.append(f\"    Target surname: {target_surname}, given: {target_given}\")\n",
        "\n",
        "    # If target has an initial, expand it to possible full names\n",
        "    target_possible_givens = []\n",
        "    target_possible_givens = [target_given]\n",
        "\n",
        "    # First try: exact surname match + given name similarity\n",
        "    for candidate in candidates:\n",
        "        # Match surname exactly\n",
        "        if normalize_text(target_surname) != normalize_text(candidate['surname']):\n",
        "            continue\n",
        "\n",
        "        # If target has an initial, check if candidate's given name matches any expansion\n",
        "        candidate_given_norm = normalize_text(candidate['given_names'])\n",
        "\n",
        "        for possible_given in target_possible_givens:\n",
        "            possible_given_norm = normalize_text(possible_given)\n",
        "\n",
        "            # Check if candidate's given name starts with any of the possible expansions\n",
        "            if candidate_given_norm.startswith(possible_given_norm):\n",
        "                debug_info.append(f\"    Initial-based match: {candidate['name']} \"\n",
        "                                  f\"(target: {target_given}, candidate: {candidate['given_names']})\")\n",
        "                return candidate\n",
        "\n",
        "            # Check if any of the possible expansions starts with candidate's given name\n",
        "            if possible_given_norm.startswith(candidate_given_norm):\n",
        "                debug_info.append(f\"    Initial-based match: {candidate['name']} \"\n",
        "                                  f\"(target: {target_given}, candidate: {candidate['given_names']})\")\n",
        "                return candidate\n",
        "\n",
        "    # Second try: fuzzy matching on full name (using the complete name from JSON)\n",
        "    best_score = 0\n",
        "    best_match = None\n",
        "    for candidate in candidates:\n",
        "        # Use the full normalized name from the database for matching\n",
        "        score = fuzz.token_set_ratio(normalize_text(target_name), candidate['normalized_name'])\n",
        "        debug_info.append(f\"    Fuzzy score for {candidate['name']}: {score}\")\n",
        "        if score > 65 and score > best_score:  # Increased threshold for better accuracy\n",
        "            best_score = score\n",
        "            best_match = candidate\n",
        "\n",
        "    if best_match:\n",
        "        debug_info.append(f\"    Fuzzy match selected: {best_match['name']} (score: {best_score})\")\n",
        "    else:\n",
        "        debug_info.append(\"    No suitable match found\")\n",
        "\n",
        "    return best_match\n",
        "\n",
        "\n",
        "def add_working_days(start_date, working_days):\n",
        "    \"\"\"Add working days excluding weekends (Saturday and Sunday)\"\"\"\n",
        "    if working_days <= 0:\n",
        "        return start_date\n",
        "\n",
        "    current_date = start_date\n",
        "    days_added = 0\n",
        "    monthday = current_date.strftime(\"%d-%m\")\n",
        "\n",
        "    while days_added < working_days:\n",
        "        current_date += timedelta(days=1)\n",
        "        # Check if it's a weekday (Monday=0, Sunday=6)\n",
        "        if (current_date.weekday() < 5) and (monthday not in holidays):  # 0-4 are Monday-Friday\n",
        "            days_added += 1\n",
        "        elif(monthday in working_holidays):\n",
        "            days_added += 1\n",
        "        else:\n",
        "            # It's a weekend, skip counting but continue to next day\n",
        "            continue\n",
        "\n",
        "    return current_date\n",
        "\n",
        "SPEC_CONFIG_FILE = 'spec_config.json'\n",
        "\n",
        "def load_spec_config():\n",
        "    \"\"\"Load specification configuration from file\"\"\"\n",
        "    try:\n",
        "        if Path(SPEC_CONFIG_FILE).exists():\n",
        "            with open(SPEC_CONFIG_FILE, 'r', encoding='utf-8') as f:\n",
        "                spec_config = json.load(f)\n",
        "                print(f\"✓ Loaded specification configuration\")\n",
        "                return spec_config\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error loading specification configuration: {e}\")\n",
        "\n",
        "    # Return default structure if file doesn't exist\n",
        "    return {\n",
        "        \"2\": {\n",
        "            \"раздела КР\": 2\n",
        "        },\n",
        "        \"3\": {\n",
        "\n",
        "        },\n",
        "        \"4\": {\n",
        "        }\n",
        "    }\n",
        "\n",
        "def get_working_days(step_text, workflow_text):\n",
        "    \"\"\"Calculate working days based on stage and specification keywords\"\"\"\n",
        "    # First check for step number\n",
        "    stage_match = re.search(r'Шаг (\\d+)', step_text)\n",
        "    if \"Утверждение\" in step_text:\n",
        "      return (2, 4, \"Stage 4\")\n",
        "    elif not stage_match:\n",
        "        return (0, 0, \"No stage number found\")\n",
        "\n",
        "    step_number = int(stage_match.group(1))\n",
        "    stage_number = step_number + 1  # Шаг 1 → stage 2, Шаг 2 → stage 3\n",
        "\n",
        "\n",
        "    # Change keywords\n",
        "    spec_config = load_spec_config()\n",
        "\n",
        "    # Default days for each stage if no keywords found\n",
        "    default_days = {\n",
        "        2: 3,\n",
        "        3: 5,\n",
        "        4: 2\n",
        "    }\n",
        "\n",
        "    # Check if this stage has configured keywords\n",
        "    if stage_number not in spec_config:\n",
        "        days = default_days.get(stage_number, 0)\n",
        "        return (days, stage_number, f\"Stage {stage_number}: not configured, using default {days} days\")\n",
        "\n",
        "    # Get keywords for this specific stage\n",
        "    stage_keywords = spec_config[stage_number]\n",
        "    workflow_lower = workflow_text.lower()\n",
        "\n",
        "    # Check each keyword for this stage\n",
        "    for keyword, days in stage_keywords.items():\n",
        "        if keyword.lower() in workflow_lower:\n",
        "            return (days, stage_number, f\"Stage {stage_number}: keyword '{keyword}' → {days} days\")\n",
        "\n",
        "    # No keywords found for this stage, use default value\n",
        "    days = default_days.get(stage_number, 0)\n",
        "    return (days, stage_number, f\"Stage {stage_number}: no keywords found, using default {days} days\")\n",
        "\n",
        "\n",
        "def extract_start_date_from_lifecycle(lifecycle_text, current_step_number):\n",
        "    \"\"\"Extract start date from lifecycle text based on previous stage completion\"\"\"\n",
        "    if not lifecycle_text or pd.isna(lifecycle_text):\n",
        "        return None\n",
        "\n",
        "\n",
        "    current_stage = current_step_number + 1\n",
        "    target_step = current_stage - 2  # Stage 3 → Шаг 1, Stage 2 → Шаг 0\n",
        "\n",
        "    if target_step < 0:\n",
        "        return None\n",
        "\n",
        "    # Pattern to find step entries with dates\n",
        "    step_pattern = rf'Шаг {target_step}.*?(\\d{{2}}\\.\\d{{2}}\\.\\d{{2}} \\d{{2}}:\\d{{2}})'\n",
        "    matches = re.findall(step_pattern, lifecycle_text, re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "    if matches:\n",
        "        # Take the last date (most recent completion)\n",
        "        last_date_str = matches[-1]\n",
        "        try:\n",
        "            # Convert from \"DD.MM.YY HH:MM\" to datetime\n",
        "            return datetime.strptime(last_date_str, '%d.%m.%y %H:%M')\n",
        "        except ValueError:\n",
        "            return None\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_date():\n",
        "    a1 = input(\"\\n\\n\\nType date in format dd.mm.YYYY \\nor pull Enter if today is ok: \")\n",
        "    if(len(a1) < 2):\n",
        "        return datetime.today().date()\n",
        "    else:\n",
        "      try:\n",
        "        return datetime.strptime(a1, \"%d.%m.%Y\").date()\n",
        "      except ValueError:\n",
        "        print(\"Date is not correct, today has been assigned as today\")\n",
        "        return datetime.today().date()\n",
        "\n",
        "\n",
        "def is_team_checked(approver_name, all_people, checked_approvers, matching_log):\n",
        "    \"\"\"Check if any team member of the given approver is already checked\"\"\"\n",
        "    # First find the best match for the approver\n",
        "    best_match = find_best_match(approver_name, all_people, matching_log)\n",
        "    if not best_match:\n",
        "        matching_log.append(f\"    No match found for team check: {approver_name}\")\n",
        "        return False\n",
        "\n",
        "    # Check if this person has a team\n",
        "    team_id = best_match.get('team_id')\n",
        "    team_emails = best_match.get('team_emails', [])\n",
        "\n",
        "    if not team_id or len(team_emails) <= 1:\n",
        "        matching_log.append(f\"    No team found for: {approver_name}\")\n",
        "        return False\n",
        "\n",
        "    matching_log.append(f\"    Checking team {team_id} with {len(team_emails)} members\")\n",
        "\n",
        "    # Find all team members in the database\n",
        "    team_members = []\n",
        "    for person in all_people:\n",
        "        if person.get('team_id') == team_id:\n",
        "            team_members.append(person)\n",
        "            matching_log.append(f\"      Team member: {person['name']} <{person['email']}>\")\n",
        "\n",
        "    # Check if any team member is in the checked approvers list\n",
        "    for team_member in team_members:\n",
        "        # Use fuzzy matching to see if team member name is in checked approvers\n",
        "        for checked_name in checked_approvers:\n",
        "            if find_best_match(checked_name, [team_member], matching_log):\n",
        "                matching_log.append(f\"    ✓ Team member {team_member['name']} is already checked\")\n",
        "                return True\n",
        "\n",
        "    matching_log.append(f\"    ✗ No team members found in checked list\")\n",
        "    return False\n",
        "\n",
        "\n",
        "def process_coordinations(df, company_person_map):\n",
        "    \"\"\"Process coordinations with disambiguation for same surnames and team handling\"\"\"\n",
        "    overdue_counts = defaultdict(int)\n",
        "    overdue_emails = []\n",
        "    overdue_coordination_ids = []\n",
        "    coordination_details = []\n",
        "    result_df = []\n",
        "    # df1 = pd.DataFrame(columns = [])\n",
        "\n",
        "\n",
        "\n",
        "    # Change today's date\n",
        "\n",
        "    # = datetime.today().date()\n",
        "\n",
        "    # = datetime.strptime(\"11.09.2025\", \"%d.%m.%Y\").date()\n",
        "\n",
        "    today = get_date()\n",
        "\n",
        "    # datetime.strptime(\"26.09.2025\", \"%d.%m.%Y\").date()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    debug_info = []\n",
        "    ambiguous_matches = []\n",
        "    matching_log = []\n",
        "\n",
        "    print(f\"\\nProcessing coordinations (as of {today})... {datetime.today()}\")\n",
        "    matching_log.append(f\"Processing coordinations (as of {today})...\")\n",
        "    matching_log.append(\"=\" * 50)\n",
        "\n",
        "    # Prepare a flat list of all people for matching\n",
        "    all_people = []\n",
        "    for company, persons in company_person_map.items():\n",
        "        for person in persons:\n",
        "            person['company'] = company\n",
        "            all_people.append(person)\n",
        "\n",
        "    # Get coordination ID column name (first column)\n",
        "    id_column = df.columns[0] if len(df.columns) > 0 else 'id'\n",
        "    print(f\"Using '{id_column}' as coordination ID\")\n",
        "    matching_log.append(f\"Using '{id_column}' as coordination ID\")\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        if not all(col in row for col in ['Не проверили на текущем шаге', 'Шаг', 'Рабочий процесс']):\n",
        "            matching_log.append(f\"Row {idx}: Missing required columns, skipping\")\n",
        "            continue\n",
        "\n",
        "        coord_id = row.get(id_column, 'N/A')\n",
        "        step_text = str(row['Шаг'])\n",
        "        workflow_text = str(row['Рабочий процесс'])\n",
        "\n",
        "        matching_log.append(f\"\\nProcessing coordination ID: {coord_id}\")\n",
        "        matching_log.append(f\"Step: {step_text}, Workflow: {workflow_text}\")\n",
        "\n",
        "        # Get working days with explanation\n",
        "        working_days, stage_number, days_explanation = get_working_days(step_text, workflow_text)\n",
        "        matching_log.append(f\"Working days calculation: {days_explanation}\")\n",
        "\n",
        "        try:\n",
        "            # First try to get start date from lifecycle (previous stage completion)\n",
        "            start_date = None\n",
        "            if 'Жизненный цикл' in row and row['Жизненный цикл']:\n",
        "                lifecycle_text = str(row['Жизненный цикл'])\n",
        "                step_match = re.search(r'Шаг (\\d+)', step_text)\n",
        "\n",
        "                if(\"Утверждение\" in step_text):\n",
        "                    current_step_number = 3\n",
        "                    start_date = extract_start_date_from_lifecycle(lifecycle_text, current_step_number)\n",
        "                elif step_match:\n",
        "                    current_step_number = int(step_match.group(1))\n",
        "\n",
        "                    start_date = extract_start_date_from_lifecycle(lifecycle_text, current_step_number)\n",
        "                    if start_date:\n",
        "                        matching_log.append(f\"  Found start date from lifecycle: {start_date}\")\n",
        "                    else:\n",
        "                        matching_log.append(\"  No valid start date found in lifecycle\")\n",
        "\n",
        "            # If no start date from lifecycle, use creation date\n",
        "            if start_date is None:\n",
        "                start_date_str = str(row['Дата и время создания согласования'])\n",
        "                start_date = datetime.strptime(start_date_str, '%Y-%m-%d %H:%M:%S')\n",
        "                matching_log.append(f\"  Using creation date as start date: {start_date}\")\n",
        "\n",
        "            deadline = add_working_days(start_date, working_days)\n",
        "            matching_log.append(f\"Start date: {start_date.date()}, Deadline: {deadline.date()}\")\n",
        "\n",
        "            # Skip if not overdue\n",
        "            if deadline.date() >= today:\n",
        "                debug_info.append({\n",
        "                    'id': coord_id,\n",
        "                    'status': 'Not overdue',\n",
        "                    'start_date': start_date.date(),\n",
        "                    'deadline': deadline.date(),\n",
        "                    'working_days': working_days,\n",
        "                    'explanation': days_explanation,\n",
        "                    'today': today\n",
        "                })\n",
        "                matching_log.append(\"Coordination is not overdue, skipping\")\n",
        "                continue\n",
        "\n",
        "        except Exception as e:\n",
        "            debug_info.append({\n",
        "                'id': coord_id,\n",
        "                'status': f'Date error: {str(e)}',\n",
        "                'start_date_str': start_date_str if 'start_date_str' in locals() else 'N/A',\n",
        "                'working_days': working_days,\n",
        "                'explanation': days_explanation\n",
        "            })\n",
        "            matching_log.append(f\"Date parsing error: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "        # Get NOT checked approvers from \"Не проверили на текущем шаге\" column\n",
        "        not_checked_text = str(row['Не проверили на текущем шаге'])\n",
        "        not_checked_approvers = [name.strip() for name in not_checked_text.split(',') if name.strip()]\n",
        "\n",
        "        matching_log.append(f\"Not checked approvers: {not_checked_approvers}\")\n",
        "\n",
        "        # Get checked approvers from \"Проверили на текущем шаге\" column\n",
        "        checked_text = str(row['Проверили на текущем шаге'])\n",
        "        checked_approvers = [name.strip() for name in checked_text.split(',') if name.strip()]\n",
        "\n",
        "        matching_log.append(f\"Checked approvers: {checked_approvers}\")\n",
        "\n",
        "        # Find email matches for not checked approvers\n",
        "        coord_emails = []\n",
        "        coord_companies = set()\n",
        "        checked_members = []\n",
        "        for approver_name in not_checked_approvers:\n",
        "            # Checking if anyone from the appover's team has already finished\n",
        "            if is_team_checked(approver_name, all_people, checked_approvers, matching_log):\n",
        "                matching_log.append(f\"    Skipping {approver_name} - team member already checked\")\n",
        "                checked_members.append(f\"{approver_name} checked!\")\n",
        "                continue\n",
        "\n",
        "\n",
        "            # Use fuzzy matching on the full name against all people\n",
        "            best_match = find_best_match(approver_name, all_people, matching_log)\n",
        "            if best_match:\n",
        "                coord_emails.append(best_match['email'])\n",
        "                coord_companies.add(best_match['company'])\n",
        "                matching_log.append(f\"    Matched: {approver_name} → {best_match['name']} <{best_match['email']}>\")\n",
        "            else:\n",
        "                matching_log.append(f\"    No match found for: {approver_name}\")\n",
        "                no_match_array.append(approver_name)\n",
        "\n",
        "        # Update overdue counts and emails\n",
        "        for company in coord_companies:\n",
        "            overdue_counts[company] += 1\n",
        "        overdue_emails.extend(coord_emails)\n",
        "        overdue_coordination_ids.append(coord_id)\n",
        "\n",
        "        # Store coordination details\n",
        "        coordination_details.append({\n",
        "            'id': coord_id,\n",
        "            'company': ', '.join(coord_companies),\n",
        "            'start_date': start_date.date(),\n",
        "            'deadline': deadline.date(),\n",
        "            'working_days': working_days,\n",
        "            'not_checked_count': len(not_checked_approvers),\n",
        "            'explanation': days_explanation,\n",
        "            'emails': coord_emails\n",
        "        })\n",
        "\n",
        "        debug_info.append({\n",
        "            'id': coord_id,\n",
        "            'status': 'Overdue',\n",
        "            'start_date': start_date.date(),\n",
        "            'deadline': deadline.date(),\n",
        "            'working_days': working_days,\n",
        "            'explanation': days_explanation,\n",
        "            'companies': list(coord_companies),\n",
        "            'checked_members' : checked_members\n",
        "        })\n",
        "        result_df.append({'id' : row['id'],\n",
        "                          'Согласование' : row['Согласование'],\n",
        "                          'Шаг' : row['Шаг'],\n",
        "                          'Срок текущего шага' : row['Срок текущего шага'],\n",
        "                          'Дата и время создания согласования' : row['Дата и время создания согласования'],\n",
        "                          'Истечение срока согласования' : row['Истечение срока согласования'],\n",
        "                          'Инициатор' : row['Инициатор'],\n",
        "                          'Согласующие' : row['Согласующие'],\n",
        "                          'Проверили на текущем шаге' : row['Проверили на текущем шаге'],\n",
        "                          'Не проверили на текущем шаге' : row['Не проверили на текущем шаге'],\n",
        "                          'Рабочий процесс' : row['Рабочий процесс'],\n",
        "                          'Жизненный цикл' : row['Жизненный цикл']\n",
        "                          })\n",
        "    result_df_out = pd.DataFrame(result_df)\n",
        "    with pd.ExcelWriter('out_result_df.xlsx',engine = 'xlsxwriter') as writer:\n",
        "        result_df_out.to_excel(writer, index = False)\n",
        "\n",
        "\n",
        "        workbook = writer.book\n",
        "        worksheet = writer.sheets['Sheet1']\n",
        "\n",
        "        worksheet.set_column('B:B', 20)\n",
        "    # result_df_out = pd.DataFrame(result_df)\n",
        "    # result_df_out.to_excel('out_result_df.xlsx', index = False, engine = 'openpyxl')\n",
        "\n",
        "    return overdue_counts, overdue_emails, overdue_coordination_ids, coordination_details, debug_info, ambiguous_matches, matching_log\n",
        "\n",
        "\n",
        "def save_results(overdue_counts, overdue_emails, overdue_coordination_ids, coordination_details, debug_info, ambiguous_matches, matching_log):\n",
        "    \"\"\"Save results to files with ambiguous matches report\"\"\"\n",
        "    try:\n",
        "        # Save company statistics\n",
        "        with open('results.txt', 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"Количество просроченных согласований: {len(overdue_coordination_ids)}\\n\\n\")\n",
        "            for company, count in sorted(overdue_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "                f.write(f\"Количество неотработанных согласований {company}: {count}\\n\")\n",
        "        print(\"Saved results.txt\")\n",
        "\n",
        "        # Save emails for reminders\n",
        "        with open('overdue_emails.txt', 'w', encoding='utf-8') as f:\n",
        "            f.write(\"\\n\".join(sorted(set(overdue_emails))))\n",
        "        print(\"Saved overdue_emails.txt\")\n",
        "\n",
        "        # Save overdue coordination IDs (convert to strings first)\n",
        "        with open('overdue_coordination_ids.txt', 'w', encoding='utf-8') as f:\n",
        "            f.write(\"\\n\".join(sorted(set(str(id) for id in overdue_coordination_ids))))\n",
        "        print(\"Saved overdue_coordination_ids.txt\")\n",
        "\n",
        "        # Save coordination details\n",
        "        with open('coordination_details.txt', 'w', encoding='utf-8') as f:\n",
        "            f.write(\"Coordination ID\\tCompany\\tStart Date\\tDeadline\\tWorking Days\\tNot Checked Count\\tExplanation\\tEmails\\n\")\n",
        "            for detail in coordination_details:\n",
        "                f.write(f\"{detail['id']}\\t{detail['company']}\\t{detail['start_date']}\\t{detail['deadline']}\\t\")\n",
        "                f.write(f\"{detail['working_days']}\\t{detail['not_checked_count']}\\t{detail['explanation']}\\t{', '.join(detail['emails'])}\\n\")\n",
        "        print(\"Saved coordination_details.txt\")\n",
        "\n",
        "        # Save coordination details to Excel file\n",
        "        coordination_df = pd.DataFrame(coordination_details)\n",
        "        if not coordination_df.empty:\n",
        "            # Reorder columns for better readability\n",
        "            coordination_df = coordination_df[['id', 'company', 'start_date', 'deadline', 'working_days',\n",
        "                                             'not_checked_count', 'explanation', 'emails']]\n",
        "            coordination_df.to_excel('coordination_details.xlsx', index=False, engine='openpyxl')\n",
        "            print(\"Saved coordination_details.xlsx\")\n",
        "\n",
        "\n",
        "\n",
        "        # Save debug info\n",
        "        with open('debug_info.txt', 'w', encoding='utf-8') as f:\n",
        "            f.write(\"Coordination ID\\tStatus\\tStart Date\\tDeadline\\tWorking Days\\tExplanation\\tCompanies\\n\\n\")\n",
        "            for info in debug_info:\n",
        "                f.write(f\"{info['id']}\\t{info['status']}\\t\")\n",
        "                if 'start_date' in info:\n",
        "                    f.write(f\"{info['start_date']}\\t{info['deadline']}\\t\")\n",
        "                else:\n",
        "                    f.write(\"N/A\\tN/A\\t\")\n",
        "                f.write(f\"{info['working_days']}\\t{info['explanation']}\\t\")\n",
        "                f.write(f\"{','.join(info.get('companies', [])) if 'companies' in info else 'N/A'}\\n\")\n",
        "        print(\"Saved debug_info.txt\")\n",
        "\n",
        "        # Save ambiguous matches for manual review\n",
        "        if ambiguous_matches:\n",
        "            with open('ambiguous_matches.txt', 'w', encoding='utf-8') as f:\n",
        "                f.write(\"Coordination ID\\tApprover Name\\tPossible Candidates\\n\")\n",
        "                for item in ambiguous_matches:\n",
        "                    candidates = \" | \".join([\n",
        "                        f\"{c['name']} <{c['email']}> ({c['company']})\"\n",
        "                        for c in item['candidates']\n",
        "                    ])\n",
        "                    f.write(f\"{item['coordination_id']}\\t{item['approver_name']}\\t{candidates}\\n\")\n",
        "            print(\"Saved ambiguous_matches.txt\")\n",
        "\n",
        "        # Save matching log\n",
        "        with open('matching_log.txt', 'w', encoding='utf-8') as f:\n",
        "            f.write(\"\\n\".join(matching_log))\n",
        "        print(\"Saved matching_log.txt\")\n",
        "\n",
        "        print(\"\\n✓ Results saved:\")\n",
        "        print(\"- results.txt (company statistics)\")\n",
        "        print(\"- overdue_emails.txt (reminder emails)\")\n",
        "        print(\"- overdue_coordination_ids.txt (overdue coordination IDs)\")\n",
        "        print(\"- coordination_details.txt (detailed coordination info)\")\n",
        "        print(\"- coordination_details.xlsx (Excel file with coordination details)\")\n",
        "        print(\"- debug_info.txt (processing debug information)\")\n",
        "        print(\"- matching_log.txt (detailed matching process)\")\n",
        "        if ambiguous_matches:\n",
        "            print(\"- ambiguous_matches.txt (matches needing manual review)\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error saving results: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "def read_file(file_path):\n",
        "    \"\"\"Read file (CSV or Excel) and return DataFrame\"\"\"\n",
        "    try:\n",
        "        if file_path.lower().endswith(('.csv', '.txt')):\n",
        "            # Try different encodings for CSV\n",
        "            try:\n",
        "                return pd.read_csv(file_path, sep=';', encoding='utf-8')\n",
        "            except UnicodeDecodeError:\n",
        "                return pd.read_csv(file_path, sep=';', encoding='windows-1251')\n",
        "        else:  # Excel\n",
        "            return pd.read_excel(file_path, engine='openpyxl')\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error reading file: {e}\")\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "\n",
        "def data_loading_mode():\n",
        "    \"\"\"Mode 1: Load and process employee data\"\"\"\n",
        "    print(\"=== Data Loading Mode ===\")\n",
        "    print(\"This mode processes employee data and updates the database.\")\n",
        "\n",
        "    # Load employee database\n",
        "    db = load_employee_db()\n",
        "\n",
        "    # Get file path from user input\n",
        "    print(\"\\nUpload company and employee data file (CSV or Excel):\")\n",
        "    file_path = upload_file(\"company and employee data\")\n",
        "    if not file_path:\n",
        "        return\n",
        "\n",
        "    # Read file\n",
        "    df = read_file(file_path)\n",
        "\n",
        "    # Convert DataFrame to text for parse_company_person_data\n",
        "    file_content = '\\n'.join(df.astype(str).values.flatten().tolist())\n",
        "\n",
        "    # Process data using enhanced matching\n",
        "    db, company_person_map, processing_log = parse_company_person_data(file_content, db)\n",
        "\n",
        "    # Save processing log\n",
        "    with open('data_loading_log.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write(\"\\n\".join(processing_log))\n",
        "    print(\"Saved data_loading_log.txt\")\n",
        "\n",
        "    print(\"\\nData loading completed successfully!\")\n",
        "    print(f\"Database now contains {len(db['employees'])} employees and {len(db['companies'])} companies\")\n",
        "\n",
        "\n",
        "def data_matching_mode():\n",
        "    \"\"\"Mode 2: Match coordination data with employee database\"\"\"\n",
        "    print(\"=== Data Matching Mode ===\")\n",
        "    print(\"This mode processes coordination data using the existing database.\")\n",
        "\n",
        "    # Load employee database\n",
        "    db = load_employee_db()\n",
        "\n",
        "    if not db['employees']:\n",
        "        print(\"No employee data found in database. Please run Data Loading mode first.\")\n",
        "        return\n",
        "\n",
        "    # Convert database to company_person_map format\n",
        "    company_person_map = defaultdict(list)\n",
        "    for employee in db['employees']:\n",
        "        company = employee['company']\n",
        "        company_person_map[company].append({\n",
        "            'name': employee['name'],\n",
        "            'email': employee['email'],\n",
        "            'normalized_name': employee['normalized_name'],\n",
        "            'surname': employee['surname'],\n",
        "            'given_names': employee['given_names'],\n",
        "            'team_id': employee.get('team_id', ''),\n",
        "            'team_emails': employee.get('team_emails', [])\n",
        "        })\n",
        "\n",
        "    # Get file path from user input\n",
        "    print(\"\\nUpload coordination data file (CSV or Excel):\")\n",
        "    file_path = upload_file(\"coordination data\")\n",
        "    if not file_path:\n",
        "        return\n",
        "\n",
        "    # Read file\n",
        "    df = read_file(file_path)\n",
        "\n",
        "    # Process coordinations\n",
        "    overdue_counts, overdue_emails, overdue_coordination_ids, coordination_details, debug_info, ambiguous_matches, matching_log = process_coordinations(\n",
        "        df, company_person_map\n",
        "    )\n",
        "\n",
        "    # Print coordination details to console for debugging\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Coordination Processing Details (Enhanced Matching):\")\n",
        "    print(\"=\" * 50)\n",
        "    for detail in coordination_details:\n",
        "        print(f\"ID: {detail['id']}\")\n",
        "        print(f\"  Company: {detail['company']}\")\n",
        "        print(f\"  Start Date: {detail['start_date']}\")\n",
        "        print(f\"  Deadline: {detail['deadline']}\")\n",
        "        print(f\"  Working Days: {detail['working_days']}\")\n",
        "        print(f\"  Explanation: {detail['explanation']}\")\n",
        "        print(f\"  Not Checked Count: {detail['not_checked_count']}\")\n",
        "        print(f\"  Emails: {', '.join(detail['emails'])}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    save_results(overdue_counts, overdue_emails, overdue_coordination_ids, coordination_details, debug_info, ambiguous_matches, matching_log)\n",
        "\n",
        "    print(\"\\nFinal results:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Overdue coordination count by company:\")\n",
        "    for company, count in sorted(overdue_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"- {company}: {count}\")\n",
        "\n",
        "    # Convert IDs to strings for display\n",
        "    overdue_ids_str = ', '.join(str(id) for id in overdue_coordination_ids)\n",
        "    print(f\"\\nOverdue coordination IDs: {overdue_ids_str}\")\n",
        "\n",
        "    if ambiguous_matches:\n",
        "        print(\"\\n⚠️ Some matches were ambiguous. Please review ambiguous_matches.txt\")\n",
        "\n",
        "    print(\"\\nData matching completed successfully!\")\n",
        "    if(no_match_array):\n",
        "      print(f\"    !!! Some People were not found in data: {set(no_match_array)}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"=== Coordination Processing System ===\")\n",
        "\n",
        "    # Check for required packages\n",
        "    try:\n",
        "        import pandas as pd\n",
        "    except ImportError:\n",
        "        print(\"Please install pandas first: pip install pandas\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        import openpyxl\n",
        "    except ImportError:\n",
        "        print(\"Please install openpyxl for Excel support: pip install openpyxl\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        from rapidfuzz import fuzz\n",
        "    except ImportError:\n",
        "        print(\"Please install rapidfuzz for fuzzy matching: pip install rapidfuzz\")\n",
        "        return\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nSelect mode:\")\n",
        "        print(\"1. Data Loading - Process employee data and update database\")\n",
        "        print(\"2. Data Matching - Process coordination data using existing database\")\n",
        "        print(\"3. Upload employee database JSON file\")\n",
        "        print(\"4. Exit\")\n",
        "\n",
        "        choice = input(\"Enter your choice (1-4): \").strip()\n",
        "\n",
        "        if choice == '1':\n",
        "            data_loading_mode()\n",
        "        elif choice == '2':\n",
        "            data_matching_mode()\n",
        "        elif choice == '3':\n",
        "            # Upload employee database\n",
        "            uploaded_file = upload_file(\"employee database JSON\")\n",
        "            if uploaded_file and uploaded_file.endswith('.json'):\n",
        "                # Copy the uploaded file to the expected location\n",
        "                import shutil\n",
        "                shutil.copy(uploaded_file, EMPLOYEE_DB_FILE)\n",
        "                print(f\"✓ Employee database uploaded and saved as {EMPLOYEE_DB_FILE}\")\n",
        "            else:\n",
        "                print(\"Please upload a valid JSON file\")\n",
        "        elif choice == '4':\n",
        "            print(\"Exiting program. Goodbye!\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Invalid choice. Please try again.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKzBknsHXWAr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgxLEVDL74Fs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}